{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark=SparkSession.builder.master(\"local[1]\").appName(\"Dataframe-creation-transformations\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-UQS9B0T.ht.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe-creation-transformations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2666f0a3e80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create dataframe from a local list or local file , we use createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_list=[(1,'2013-07-25',11599,'CLOSED'),(2,'2013-07-25',256,'PENDING'),(3,'2013-07-25',12111,'COMPLETE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df=Spark.createDataFrame(order_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+--------+\n",
      "| _1|        _2|   _3|      _4|\n",
      "+---+----------+-----+--------+\n",
      "|  1|2013-07-25|11599|  CLOSED|\n",
      "|  2|2013-07-25|  256| PENDING|\n",
      "|  3|2013-07-25|12111|COMPLETE|\n",
      "+---+----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inorder to add header we can use toDF() , which is one method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df1=orders_df.toDF('order_id','order_date','customer_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "|       1|2013-07-25|      11599|      CLOSED|\n",
      "|       2|2013-07-25|        256|     PENDING|\n",
      "|       3|2013-07-25|      12111|    COMPLETE|\n",
      "+--------+----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method is that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema='order_id long,order_date string,customer_id long,order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df_schema=Spark.createDataFrame(order_list,order_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df_schema_new=orders_df_schema.withColumn(\"order_date\",to_timestamp(\"order_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df_schema_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to convert rdd to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=Spark.sparkContext.textFile(r\"C:\\Users\\user\\OneDrive\\Desktop\\Big-data-trendytech\\SAMPLE_ORDERS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,2013-07-25,11599,CLOSED',\n",
       " '2,2013-07-26,256,COMPLETE',\n",
       " '3,2013-07-27,12111,CLOSED',\n",
       " '4,2013-08-28,8827,PENDING',\n",
       " '5,2013-07-29,11318,COMPLETE']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_map=rdd.map(lambda x:(int(x.split(',')[0]),x.split(',')[1],int(x.split(',')[2]),x.split(',')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '2013-07-25', 11599, 'CLOSED'),\n",
       " (2, '2013-07-26', 256, 'COMPLETE'),\n",
       " (3, '2013-07-27', 12111, 'CLOSED'),\n",
       " (4, '2013-08-28', 8827, 'PENDING'),\n",
       " (5, '2013-07-29', 11318, 'COMPLETE')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two ways we can convert rdd to dataframe\n",
    "#1.using createDataframe\n",
    "#2.using toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m1=Spark.createDataFrame(rdd_map,order_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m2=rdd_map.toDF(order_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=Spark.sparkContext.textFile(r\"C:\\Users\\user\\OneDrive\\Desktop\\Big-data-trendytech\\udemy\\RetailDB+SalesData\\RetailDB SalesData\\Order_items\\part-00000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,1,957,1,299.98,299.98',\n",
       " '2,2,1073,1,199.99,199.99',\n",
       " '3,2,502,5,250.0,50.0',\n",
       " '4,2,403,1,129.99,129.99',\n",
       " '5,4,897,2,49.98,24.99']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_map=rdd.map(lambda x:(int(x.split(',')[0]),int(x.split(',')[1]),int(x.split(',')[2]),int(x.split(',')[3]),float(x.split(',')[4]),float(x.split(',')[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 957, 1, 299.98, 299.98),\n",
       " (2, 2, 1073, 1, 199.99, 199.99),\n",
       " (3, 2, 502, 5, 250.0, 50.0),\n",
       " (4, 2, 403, 1, 129.99, 129.99),\n",
       " (5, 4, 897, 2, 49.98, 24.99)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_map.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_schema=\"order_item_id long,order_id long,product_id long,quantity int,subtotal float,product_price float\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdt=Spark.createDataFrame(rdd_map,product_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- subtotal: float (nullable = true)\n",
      " |-- product_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pdt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "|order_item_id|order_id|product_id|quantity|subtotal|product_price|\n",
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "|            1|       1|       957|       1|  299.98|       299.98|\n",
      "|            2|       2|      1073|       1|  199.99|       199.99|\n",
      "|            3|       2|       502|       5|   250.0|         50.0|\n",
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pdt.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column subtotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df_pdt=df_pdt.drop(\"subtotal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- product_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_df_pdt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+--------+-------------+---------+\n",
      "|order_item_id|order_id|product_id|quantity|product_price|subtotal1|\n",
      "+-------------+--------+----------+--------+-------------+---------+\n",
      "|            1|       1|       957|       1|       299.98|   299.98|\n",
      "|            2|       2|      1073|       1|       199.99|   199.99|\n",
      "+-------------+--------+----------+--------+-------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_df_pdt_new=drop_df_pdt.select(\"*\",expr(\"product_price*quantity as subtotal1\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we have more than one calculation , we can use selectExpr instead of  select and then expr infront of the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+--------+-------------+---------+\n",
      "|order_item_id|order_id|product_id|quantity|product_price|subtotal1|\n",
      "+-------------+--------+----------+--------+-------------+---------+\n",
      "|            1|       1|       957|       1|       299.98|   299.98|\n",
      "|            2|       2|      1073|       1|       199.99|   199.99|\n",
      "+-------------+--------+----------+--------+-------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product=drop_df_pdt.selectExpr(\"*\",\"product_price*quantity as subtotal1\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdt_wth_name=Spark.sparkContext.textFile(r\"C:\\Users\\user\\OneDrive\\Desktop\\Big-data-trendytech\\udemy\\RetailDB+SalesData\\RetailDB SalesData\\Order_items\\part-00000 - name.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdt_wth_name_map=pdt_wth_name.map(lambda x:(int(x.split(',')[0]),int(x.split(',')[1]),int(x.split(',')[2]),int(x.split(',')[3]),float(x.split(',')[4]),float(x.split(',')[5]),x.split(',')[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_schema_name=\"order_item_id long,order_id long,product_id long,quantity int,subtotal float,product_price float,pdt_name string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdt_wth_name_map=Spark.createDataFrame(pdt_wth_name_map,product_schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+--------+--------+-------------+----------------+\n",
      "|order_item_id|order_id|product_id|quantity|subtotal|product_price|        pdt_name|\n",
      "+-------------+--------+----------+--------+--------+-------------+----------------+\n",
      "|            1|       1|       957|       1|  299.98|       299.98|        Nike dfs|\n",
      "|            2|       2|      1073|       1|  199.99|       199.99|fdg Armour ghdhd|\n",
      "+-------------+--------+----------+--------+--------+-------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pdt_wth_name_map.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to increase the price of each item 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2=df_pdt_wth_name_map.withColumn(\"product_price\",expr(\"product_price*1.2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+--------+--------+------------------+----------------+\n",
      "|order_item_id|order_id|product_id|quantity|subtotal|     product_price|        pdt_name|\n",
      "+-------------+--------+----------+--------+--------+------------------+----------------+\n",
      "|            1|       1|       957|       1|  299.98|359.97601318359375|        Nike dfs|\n",
      "|            2|       2|      1073|       1|  199.99|239.98800659179688|fdg Armour ghdhd|\n",
      "+-------------+--------+----------+--------+--------+------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I need to increase 20 percent for Nike products,10% for Armour products and no change for other products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df3=df_pdt_wth_name_map.withColumn(\"product_price\",expr(\"CASE WHEN pdt_name like '%Nike%' THEN product_price*1.2 WHEN pdt_name like '%ARMOUR%' THEN product_price*1.1 ELSE product_price END\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+--------+--------+------------------+----------------+\n",
      "|order_item_id|order_id|product_id|quantity|subtotal|     product_price|        pdt_name|\n",
      "+-------------+--------+----------+--------+--------+------------------+----------------+\n",
      "|            1|       1|       957|       1|  299.98|359.97601318359375|        Nike dfs|\n",
      "|            2|       2|      1073|       1|  199.99|199.99000549316406|fdg Armour ghdhd|\n",
      "|            3|       2|       502|       5|   250.0|              50.0|fdg Armour ghdhd|\n",
      "|            4|       2|       403|       1|  129.99|129.99000549316406|fdg Armour ghdhd|\n",
      "|            5|       4|       897|       2|   49.98|29.987999725341794|        Nike dfs|\n",
      "+-------------+--------+----------+--------+--------+------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to remove duplicate records from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist=[(1,\"kapil\",34),(1,\"kapil\",34),(1,\"satish\",26),(2,\"satish\",26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_df=Spark.createDataFrame(mylist).toDF(\"id\",\"name\",\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1| kapil| 34|\n",
      "|  1| kapil| 34|\n",
      "|  1|satish| 26|\n",
      "|  2|satish| 26|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rough_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rough_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use distinct() to remove duplicates but it will remove records if all records are duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_df1=rough_df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1|satish| 26|\n",
      "|  2|satish| 26|\n",
      "|  1| kapil| 34|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rough_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rough_df.select(\"id\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way of removing duplicates is dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1|satish| 26|\n",
      "|  2|satish| 26|\n",
      "|  1| kapil| 34|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rough_df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1| kapil| 34|\n",
      "|  1|satish| 26|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rough_df.dropDuplicates([\"name\",\"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
